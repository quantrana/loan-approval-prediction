# üè¶ Loan-Approval Prediction ‚Äì Credit-Risk Insight (KNN & RF **from scratch**)

Accurate, interpretable loan decisions sit at the heart of every lender‚Äôs risk engine.
This repository shows an **end-to-end machine-learning pipeline**‚Äîfrom CSV ingestion to a hand-coded Random Forest that reaches **ROC-AUC ‚âà 0 ¬∑ 89**‚Äîbuilt for Assignment 2 of *Introduction to AI* @ UTS.

> **Key twist:** *Both* K-Nearest Neighbors **and** Random Forest are implemented **entirely from scratch** in pure Python (Numpy), with no scikit-learn shortcuts.‚Äç

---

## Table of Contents

1. [Problem Statement](#problem-statement)
2. [Dataset](#dataset)
3. [Solution Pipeline](#solution-pipeline)
4. [Why These Design Choices?](#why-these-design-choices)
5. [Experimental Results](#experimental-results)
6. [Future Work](#future-work)

---

## Problem Statement

Predict binary target **`loan_status`** ( `1 = approved`, `0 = declined` ) using 12 applicant, credit-history and loan attributes‚Äîbalancing default risk against customer inclusivity.

---

## Dataset

* **Source**  Kaggle ‚ÄúPS4E9 ‚îÇ Loan Approval Prediction‚Äù.
* **Size**    32  581 rows √ó 12 columns.
* **Mix**     8 numerical ‚ñ™ 4 categorical.
* **Imbalance**  ‚âà 24 % approvals.&#x20;

---

## Solution Pipeline

| Stage                   | Key Steps                                                                                                       |
| ----------------------- | --------------------------------------------------------------------------------------------------------------- |
| **EDA**                 | Descriptive stats, correlation matrix.                                                                          |
| **Missing Values**      | Mode fill (categoricals) ‚ûú **KNNImputer** (k = 5) for numerics.                                                 |
| **Outliers**            | IQR capping (winsorize at 1.5 √ó IQR).                                                                           |
| **Scaling**             | `StandardScaler` on all numerics.                                                                               |
| **Encoding**            | One-Hot (drop-first) for 4 categorical cols.                                                                    |
| **Split**               | 80 / 20 stratified train‚Äìtest.                                                                                  |
| **Re-balancing**        | **SMOTE** ‚ü∂ only on training set.                                                                               |
| **Modelling (Scratch)** | ‚ë† **Custom K-NN** (majority vote, Euclidean) ‚ë° **Custom Random Forest** (bootstrap, CART splits, majority vote) |
| **Tuning**              | 10-draw **randomised search** over tree depth, estimators, split criteria.                                      |
| **Evaluation**          | Accuracy ‚ñ™ weighted Precision/Recall/F1 ‚ñ™ Confusion Matrix ‚ñ™ ROC-AUC.                                           |

---

## Why These Design Choices?

| Decision                          | Rationale                                                                                        |
| --------------------------------- | ------------------------------------------------------------------------------------------------ |
| **Pure-Python KNN & RF**          | Reinforces algorithmic understanding; avoids hidden scikit-learn heuristics; easy to instrument. |
| **SMOTE on train only**           | Prevents leakage; lifts minority-class F1 by \~4 pp.                                             |
| **Randomised search (10 combos)** | 90 % less compute than full grid with negligible performance loss.                               |
| **StandardScaler**                | Required for distance-based KNN and stable split thresholds.                                     |

---

## Experimental Results

| Model (scratch)           |   Accuracy | Weighted F1 |    ROC-AUC |
| ------------------------- | ---------: | ----------: | ---------: |
| **K-NN (k = 3)**          |     0 ¬∑ 79 |      0 ¬∑ 79 |     0 ¬∑ 84 |
| **Random Forest (base)**  |     0 ¬∑ 81 |      0 ¬∑ 81 |     0 ¬∑ 87 |
| **Random Forest (tuned)** | **0 ¬∑ 83** |  **0 ¬∑ 83** | **0 ¬∑ 89** |

The tuned forest (100 trees, depth 10, `max_features='sqrt'`) cut false approvals by 8 % while boosting recall on true approvals by 6 %.

## Future Work

* **Cost-Sensitive Forest** ‚Äì integrate class-weighting in node impurities.
* **Explainability** ‚Äì SHAP values for per-applicant transparency.
* **Deployment** ‚Äì FastAPI + Docker to serve real-time approvals.
* **MLOps** ‚Äì add CI tests comparing scratch models vs scikit-learn baselines.

